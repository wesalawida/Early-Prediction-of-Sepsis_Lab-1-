{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ea48c5",
   "metadata": {},
   "source": [
    "# Predection :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a5378",
   "metadata": {},
   "source": [
    "## libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b321295-3c4d-4115-8648-15ada8f35886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from random import seed\n",
    "import tarfile\n",
    "seed(1121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9536c1d3-39f1-4e15-ada4-11c83c35078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning methods\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import  SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import f1_score,balanced_accuracy_score,accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fa7245-cd7b-443f-b388-24a164b56036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c62bdc",
   "metadata": {},
   "source": [
    "## Load and Preprocessing Data :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b53e7",
   "metadata": {},
   "source": [
    "load data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e7dce-2ca7-41b0-8497-f909894ee34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the dtd from tar file\n",
    "tf = tarfile.open('lab 1/data.tar')\n",
    "tf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "033fc150-dda1-420d-bef5-fe52322671e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'lab 1/data/train'\n",
    "test_data_path = 'lab 1/data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb4723aa-2eed-4061-889e-45179772473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num training patients: 20000\n",
      "num test patients: 10000\n"
     ]
    }
   ],
   "source": [
    "#get the ids of the patients from the folders :\n",
    "tr_patients = [p for p in sorted(os.listdir(train_data_path))]\n",
    "ts_patients = [p for p in sorted(os.listdir(test_data_path))]\n",
    "\n",
    "print('num training patients:', len(tr_patients))\n",
    "print('num test patients:', len(ts_patients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daa9ad94-1afd-4607-915f-4e15025ebe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_patients(dirr ,patients):\n",
    "    toconcat = []\n",
    "    for p in patients:\n",
    "        df = pd.read_csv(dirr + '/' + p, sep = \"|\")\n",
    "        e=df.shape[0] \n",
    "        patient_id = [p[8:-4]]* e\n",
    "        df[\"patient_id\"] = patient_id\n",
    "        toconcat.append(df)\n",
    "    dff = pd.concat(toconcat)\n",
    "    return dff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ec75e5d-d164-4d30-9024-aedfa65f4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = concat_patients(train_data_path, tr_patients)\n",
    "test_df = concat_patients(test_data_path, ts_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493575f-0b36-4ff8-a115-b2146b422f30",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08c021b6-1a61-41a5-af2b-5c59425f836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train=train_df.drop(['SepsisLabel','patient_id'],axis=1)\n",
    "features_test=test_df.drop(['SepsisLabel','patient_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71edf79e-01d0-4415-b4b1-b12f6292a721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  drop columns as mentioned at the feature engineering part :\n",
    "features_train = features_train[['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'Glucose', 'Age', 'Gender', 'HospAdmTime', 'EtCO2']]\n",
    "features_test = features_test[['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'Glucose', 'Age', 'Gender', 'HospAdmTime', 'EtCO2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a534e9f7-c589-4c2f-9d12-97cf7f5135bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here complet the missing data using fillna and the mean of the data\n",
    "\n",
    "features_train=features_train.fillna(features_train.mean())\n",
    "features_test=features_test.fillna(features_test.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c2f7099-2587-4769-ad61-466897da0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurearr_train=features_train.iloc[:,:].values\n",
    "featurearr_test=features_test.iloc[:,:].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4ccc309-a75b-4e4b-96bd-83906105b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train=train_df[['SepsisLabel',\"patient_id\"]]\n",
    "label_test=test_df[['SepsisLabel',\"patient_id\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ce45a26-e61c-48ea-88c4-a89c8d381db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_arr=label_train.iloc[:].values\n",
    "y_test_arr=label_test.iloc[:].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a8f5b40-38f9-427a-88f1-b5146e1a97dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "patientid_train=train_df['patient_id'].values.tolist()\n",
    "patientid_test=test_df['patient_id'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89595035-6620-44aa-814d-f410f67ecbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending Hourly data of each patent to single row:\n",
    "\n",
    "def append_data(patientid_t,featurearr_t,yarr_t):\n",
    "    c=0\n",
    "    ip=[]\n",
    "    yp=[]\n",
    "    seq=[]\n",
    "    index=0\n",
    "    for i in range(len(patientid_t)):\n",
    "        if i==0:\n",
    "            seq+=featurearr_t[index].tolist()\n",
    "            index+=1\n",
    "            continue\n",
    "        if patientid_t[i]!=patientid_t[i-1]:\n",
    "            ip.append(seq)\n",
    "            yp+=[yarr_t[i-1,0]]\n",
    "            seq=[]\n",
    "        if yarr_t[i,0]==0:\n",
    "            seq+=featurearr_t[index].tolist()\n",
    "        elif yarr_t[i,0]==1 and patientid_t[i-1]!=patientid_t[i]:\n",
    "            seq+=featurearr_t[index].tolist()\n",
    "        index+=1\n",
    "    ip.append(seq)\n",
    "    yp+=[yarr_t[i-1,0]]\n",
    "    return ip,yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "040bae01-1767-4a1c-9b18-9a5da5b52824",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp_train, yp_train = append_data(patientid_train,featurearr_train,y_train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a21e61fe-d94d-4f38-a860-0c0c976e81cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp_test, yp_test = append_data(patientid_test,featurearr_test,y_test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac40aacf-ce08-4877-b518-a3e907c6bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_test = []\n",
    "for i in range(len(patientid_test)):\n",
    "    if patientid_test[i]!=patientid_test[i-1]:\n",
    "        ids_test.append(patientid_test[i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09d87b76-d391-407d-826e-76b6b08b3f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Patients Diagnosed With Sepsis in the train set is 1415\n"
     ]
    }
   ],
   "source": [
    "# get the number of the Patients that have Sepsis in the train set \n",
    "Sepsis_train=0\n",
    "for i in yp_train:\n",
    "    if i==1:\n",
    "        Sepsis_train+=1\n",
    "print('Number of Patients Diagnosed With Sepsis in the train set is',Sepsis_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3fc6f58-4103-4c59-aa11-9f9ac3a286ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Patients Diagnosed With Sepsis in the test set is 741\n"
     ]
    }
   ],
   "source": [
    "# get the number of the Patients that have Sepsis in the test set \n",
    "Sepsis_test=0\n",
    "for i in yp_test:\n",
    "    if i==1:\n",
    "        Sepsis_test+=1\n",
    "print('Number of Patients Diagnosed With Sepsis in the test set is',Sepsis_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90978bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array(xp_train)\n",
    "y_train=np.array(yp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef8483",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.array(xp_test)\n",
    "y_test=np.array(yp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c815d149-9657-4e2f-b8eb-9cae287b2021",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=4032)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=4032)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a73b785-5228-4f0a-adac-bbe97682cde7",
   "metadata": {},
   "source": [
    "# Training/Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181e1e2-3c4d-40d6-b5ed-978f41eb0b2e",
   "metadata": {},
   "source": [
    "## RandomForestClassifier  :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da7543-2803-48fe-8e7c-13699cd27945",
   "metadata": {},
   "source": [
    "### hyperparameter of RandomForestClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8ad6714-52a1-43a0-9760-0fcdd4a133a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "print(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb04ab91-fec8-409a-9a11-595bac21b2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [100, 575, 1050, 1525, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 35, 60, 85, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "529badbd-cf4c-4c82-87fe-934e6161f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c79083b7-6847-428e-a2f6-b807fa9ac5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search of parameters, using 3 fold cross validation,\n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, verbose=2,cv = 3, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292be467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random search model\n",
    "rf_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1dad4-5043-497a-b59c-367296f59327",
   "metadata": {},
   "source": [
    "### Base Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35f3a3db-d477-430c-af1c-bd0c7f12a795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.962\n",
      "F1 Score: 0.693\n",
      "Balanced Accuracy Score: 0.786\n"
     ]
    }
   ],
   "source": [
    "rf_base = RandomForestClassifier()\n",
    "rf_base.fit(x_train, y_train)\n",
    "y_RF1= rf_base.predict(x_test)\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_RF1))\n",
    "print('F1 Score: %.3f' % f1_score(y_test, y_RF1))\n",
    "print('Balanced Accuracy Score: %.3f' % balanced_accuracy_score(y_test, y_RF1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd78c63-e4a9-49a5-bb79-be65595ea4b3",
   "metadata": {},
   "source": [
    "### Best Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99329129-b717-4ccf-87c3-3551a9ec54a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 575,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': None,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "066013bd-4596-47e2-998d-bf0d0de895a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.962\n",
      "F1 Score: 0.691\n",
      "Balanced Accuracy Score: 0.785\n"
     ]
    }
   ],
   "source": [
    "rf_best = RandomForestClassifier(n_estimators= 575,min_samples_split= 2,min_samples_leaf= 2,max_features= 'auto',max_depth = None, bootstrap =  True)\n",
    "\n",
    "rf_best.fit(x_train, y_train)\n",
    "y_RF2= rf_best.predict(x_test)\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_RF2))\n",
    "print('F1 Score: %.3f' % f1_score(y_test, y_RF2))\n",
    "print('Balanced Accuracy Score: %.3f' % balanced_accuracy_score(y_test, y_RF2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1b6b4-8f2a-46b3-acd6-60c32130ac5f",
   "metadata": {},
   "source": [
    "## MLPClassifier :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b572068",
   "metadata": {},
   "source": [
    "### hyperparameter of MLPClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feaec96-4918-45b3-84ec-21b67bf5101d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 200, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "\n",
    "# Look at parameters used by our current MLPClassifier\n",
    "print('Parameters currently in use:\\n')\n",
    "print(mlp.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a0a76a-926e-4358-89ce-70149ac23d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': [(10, 30, 10), (20,), (50, 50, 50, 50), (100,)], 'activation': ['tanh', 'relu'], 'solver': ['sgd', 'adam'], 'alpha': [0.0001, 0.05], 'learning_rate': ['constant', 'adaptive']}\n"
     ]
    }
   ],
   "source": [
    "#The ith element represents the number of neurons in the ith hidden layer:\n",
    "hidden_layer_sizes =  [(10,30,10),(20,),(50,50,50,50) ,(100,)]\n",
    "#Activation function for the hidden layer:\n",
    "activation = ['tanh', 'relu']\n",
    "#The solver for weight optimization:\n",
    "solver = ['sgd', 'adam']\n",
    "#Strength of the L2 regularization term:\n",
    "alpha = [0.0001, 0.05]\n",
    "#Learning rate schedule for weight updates:\n",
    "learning_rate = ['constant','adaptive']\n",
    "\n",
    "# Create the random grid\n",
    "MLP_random_grid = {'hidden_layer_sizes': hidden_layer_sizes,\n",
    "                   'activation': activation,\n",
    "                   'solver': solver,\n",
    "                   'alpha': alpha,\n",
    "                   'learning_rate': learning_rate}\n",
    "\n",
    "print(MLP_random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ded49-1f4d-49d8-986e-a18d374d1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "mlp = MLPClassifier()\n",
    "# MLPClassifier of parameters, using 3 fold cross validation,\n",
    "# search across 10 different combinations, and use all available cores\n",
    "mlp_random = RandomizedSearchCV(estimator = mlp, param_distributions = MLP_random_grid, n_iter = 10, verbose=2,cv = 3, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02a55b74-4a01-4adb-8bcc-2bf04341eab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 10.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                           batch_size='auto', beta_1=0.9,\n",
       "                                           beta_2=0.999, early_stopping=False,\n",
       "                                           epsilon=1e-08,\n",
       "                                           hidden_layer_sizes=(100,),\n",
       "                                           learning_rate='constant',\n",
       "                                           learning_rate_init=0.001,\n",
       "                                           max_fun=15000, max_iter=200,\n",
       "                                           momentum=0.9, n_iter_no_change=10,\n",
       "                                           nesterovs_momentum=True, power_t=0.5,\n",
       "                                           random...\n",
       "                                           verbose=False, warm_start=False),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=-1,\n",
       "                   param_distributions={'activation': ['tanh', 'relu'],\n",
       "                                        'alpha': [0.0001, 0.05],\n",
       "                                        'hidden_layer_sizes': [(10, 30, 10),\n",
       "                                                               (20,),\n",
       "                                                               (50, 50, 50, 50),\n",
       "                                                               (100,)],\n",
       "                                        'learning_rate': ['constant',\n",
       "                                                          'adaptive'],\n",
       "                                        'solver': ['sgd', 'adam']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306d898-70ce-46fa-bbbc-a97ae0a8a91a",
   "metadata": {},
   "source": [
    "### Bese Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a031f3b-3da3-4f33-982d-248cdfdccccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.961\n",
      "F1 Score: 0.675\n",
      "Balanced Accuracy Score: 0.773\n"
     ]
    }
   ],
   "source": [
    "MLP_base = MLPClassifier()\n",
    "MLP_base.fit(x_train, y_train)\n",
    "y_MLP1 = MLP_base.predict(x_test)\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_MLP1))\n",
    "print('F1 Score: %.3f' % f1_score(y_test, y_MLP1))\n",
    "print('Balanced Accuracy Score: %.3f' % balanced_accuracy_score(y_test, y_MLP1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a936f0f-62fb-42a4-b140-f3ed9b17ef1a",
   "metadata": {},
   "source": [
    "### Best Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01eabfc2-03a9-4d05-9ef6-d3e7ff418a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'adam',\n",
       " 'learning_rate': 'constant',\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'alpha': 0.05,\n",
       " 'activation': 'relu'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a97e96a7-8896-40d3-99b2-eb94eb9f7feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.961\n",
      "F1 Score: 0.685\n",
      "Balanced Accuracy Score: 0.784\n"
     ]
    }
   ],
   "source": [
    "MLP_best = MLPClassifier(solver= 'adam',\n",
    " learning_rate= 'constant',\n",
    " hidden_layer_sizes= (100,),\n",
    " alpha= 0.05,\n",
    " activation= 'relu')\n",
    "\n",
    "MLP_best.fit(x_train, y_train)\n",
    "y_MLP2= MLP_best.predict(x_test)\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_MLP2))\n",
    "print('F1 Score: %.3f' % f1_score(y_test,y_MLP2))\n",
    "print('Balanced Accuracy Score: %.3f' % balanced_accuracy_score(y_test, y_MLP2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b6c35-7221-47f5-a3c1-a7105be8504d",
   "metadata": {},
   "source": [
    "##  Different models we tried : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b8655",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c8613e5-8c77-4fbc-8d37-7c1c76591ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.955\n",
      "F1 Score: 0.583\n",
      "Balanced Accuracy Score: 0.712\n"
     ]
    }
   ],
   "source": [
    "sgdc = SGDClassifier(loss='hinge',max_iter=2000,tol=0.001)\n",
    "sgdc.fit(x_train,y_train)\n",
    "sgdc_preds = sgdc.predict(x_test)\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, sgdc_preds))\n",
    "print('F1 Score: %.3f' % f1_score(y_test, sgdc_preds))\n",
    "print('Balanced Accuracy Score: %.3f' % balanced_accuracy_score(y_test,sgdc_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e5687",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cba5783d-0f01-4c72-a497-6d3e6493779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.880\n",
      "F1 Score: 0.420\n",
      "Balanced Accuracy Score: 0.745\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train,y_train)\n",
    "dt_preds = dt.predict(x_test)\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, dt_preds))\n",
    "print('F1 Score: %.3f' % f1_score(y_test, dt_preds))\n",
    "print('Balanced Accuracy Score: %.3f' % balanced_accuracy_score(y_test, dt_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a5ce0-9faa-4d6d-99d6-4d81e8386f9e",
   "metadata": {},
   "source": [
    "### CREAT CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6fca951a-9b13-44ff-9fa8-3ab3dc37c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict = {'Ids':ids_test, 'SepsisLabel':y_MLP2} \n",
    "df = pd.DataFrame(list_dict) \n",
    "df.to_csv('prediction.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ba825",
   "metadata": {},
   "source": [
    "### take the best model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd27290c-6296-4dca-ba29-dd9587c2f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a955da9-7e42-417b-8ba0-69f84cc36b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(rf_best, 'model.joblib') \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
